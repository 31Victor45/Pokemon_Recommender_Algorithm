{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f537893",
   "metadata": {},
   "source": [
    "# ¿Qué es la similitud del coseno?\n",
    "\n",
    "La **similitud del coseno** es una métrica empleada para cuantificar la similitud entre dos vectores no nulos en un espacio de producto interior. Se define como el coseno del ángulo entre ellos. Es particularmente útil en el procesamiento de lenguaje natural y en la recuperación de información para medir la similitud entre documentos o textos, ya que su cálculo no se ve afectado por la magnitud de los vectores, sino únicamente por su dirección. Esto permite que documentos de diferentes longitudes pero con temáticas similares sean considerados afines.\n",
    "\n",
    "---\n",
    "\n",
    "### Fórmula Matemática\n",
    "\n",
    "La fórmula para calcular la similitud del coseno entre dos vectores $A$ y $B$ es la siguiente:\n",
    "\n",
    "$$\\text{similitud}(A, B) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|} = \\frac{\\sum_{i=1}^{n} A_i B_i}{\\sqrt{\\sum_{i=1}^{n} A_i^2} \\sqrt{\\sum_{i=1}^{n} B_i^2}}$$\n",
    "\n",
    "Donde:\n",
    "* $A \\cdot B$: Es el **producto escalar** (o producto punto) de los vectores $A$ y $B$.\n",
    "* $\\|A\\|$: Representa la **norma euclidiana** (o magnitud) del vector $A$.\n",
    "* $\\|B\\|$: Representa la **norma euclidiana** (o magnitud) del vector $B$.\n",
    "* $A_i$ y $B_i$: Son los componentes $i$-ésimos de los vectores $A$ y $B$, respectivamente.\n",
    "\n",
    "---\n",
    "\n",
    "### Interpretación de los Resultados\n",
    "\n",
    "El valor de la similitud del coseno se encuentra siempre en el rango de -1 a 1:\n",
    "\n",
    "* **1:** Indica que los vectores son idénticos en dirección, lo que se traduce en una **similitud perfecta**.\n",
    "* **0:** Indica que los vectores son ortogonales (perpendiculares), es decir, no tienen relación alguna o son completamente independientes.\n",
    "* **-1:** Indica que los vectores tienen direcciones completamente opuestas, lo que implica una **total disimilitud**.\n",
    "\n",
    "En contextos como el procesamiento de lenguaje natural, donde los vectores a menudo representan frecuencias de palabras y sus componentes son no negativos, la similitud del coseno generalmente se encuentra en el rango de 0 a 1, dado que los ángulos entre vectores de este tipo no suelen superar los 90 grados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fee45cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector A: [1 1 0 1 0 1]\n",
      "Vector B: [1 1 1 0 1 0]\n",
      "\n",
      "Producto escalar (A . B): 2\n",
      "Magnitud de Vector A (||A||): 2.0000\n",
      "Magnitud de Vector B (||B||): 2.0000\n",
      "\n",
      "Similitud del Coseno: 0.5000\n",
      "\n",
      "Similitud del Coseno (usando función): 0.5000\n",
      "\n",
      "Vector C: [1 2 3]\n",
      "Vector D: [2 4 6]\n",
      "Similitud Coseno (C, D): 1.0000\n",
      "\n",
      "Vector E: [1 0 0]\n",
      "Vector F: [0 1 0]\n",
      "Similitud Coseno (E, F): 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Definimos dos vectores de ejemplo\n",
    "# En el contexto de PNL, estos podrían ser representaciones vectoriales\n",
    "# (embeddings) de documentos o palabras.\n",
    "vector_a = np.array([1, 1, 0, 1, 0, 1])\n",
    "vector_b = np.array([1, 1, 1, 0, 1, 0])\n",
    "\n",
    "print(f\"Vector A: {vector_a}\")\n",
    "print(f\"Vector B: {vector_b}\")\n",
    "\n",
    "# Paso 1: Calcular el producto escalar (dot product) de los dos vectores\n",
    "# Fórmula: A . B = sum(Ai * Bi)\n",
    "dot_product = np.dot(vector_a, vector_b)\n",
    "print(f\"\\nProducto escalar (A . B): {dot_product}\")\n",
    "\n",
    "# Paso 2: Calcular la magnitud (norma euclidiana) de cada vector\n",
    "# Fórmula: ||A|| = sqrt(sum(Ai^2))\n",
    "norm_a = np.linalg.norm(vector_a)\n",
    "norm_b = np.linalg.norm(vector_b)\n",
    "print(f\"Magnitud de Vector A (||A||): {norm_a:.4f}\")\n",
    "print(f\"Magnitud de Vector B (||B||): {norm_b:.4f}\")\n",
    "\n",
    "# Paso 3: Calcular la similitud del coseno\n",
    "# Fórmula: similitud(A, B) = (A . B) / (||A|| * ||B||)\n",
    "# Se añade una verificación para evitar división por cero si alguna norma es 0\n",
    "if norm_a == 0 or norm_b == 0:\n",
    "    cosine_similarity = 0.0\n",
    "    print(\"\\nAl menos uno de los vectores es un vector cero, la similitud del coseno es 0.\")\n",
    "else:\n",
    "    cosine_similarity = dot_product / (norm_a * norm_b)\n",
    "    print(f\"\\nSimilitud del Coseno: {cosine_similarity:.4f}\")\n",
    "\n",
    "# Un enfoque más compacto y común es usar una función:\n",
    "def calcular_similitud_coseno(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "\n",
    "    if norm_vec1 == 0 or norm_vec2 == 0:\n",
    "        return 0.0\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "print(f\"\\nSimilitud del Coseno (usando función): {calcular_similitud_coseno(vector_a, vector_b):.4f}\")\n",
    "\n",
    "\n",
    "# Ejemplo con vectores que deberían tener alta similitud (dirección similar)\n",
    "vector_c = np.array([1, 2, 3])\n",
    "vector_d = np.array([2, 4, 6]) # Este es un múltiplo de vector_c\n",
    "print(f\"\\nVector C: {vector_c}\")\n",
    "print(f\"Vector D: {vector_d}\")\n",
    "print(f\"Similitud Coseno (C, D): {calcular_similitud_coseno(vector_c, vector_d):.4f}\") # Debería ser 1.0\n",
    "\n",
    "# Ejemplo con vectores con baja similitud (ortogonales o casi)\n",
    "vector_e = np.array([1, 0, 0])\n",
    "vector_f = np.array([0, 1, 0])\n",
    "print(f\"\\nVector E: {vector_e}\")\n",
    "print(f\"Vector F: {vector_f}\")\n",
    "print(f\"Similitud Coseno (E, F): {calcular_similitud_coseno(vector_e, vector_f):.4f}\") # Debería ser 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ba865a",
   "metadata": {},
   "source": [
    "# ¿Los elementos individuales de los vectores tienen que estar normalizados?\n",
    "\n",
    "---\n",
    "\n",
    "Cuando hablamos de \"elementos\" de un vector en el contexto de la similitud del coseno, nos referimos a los valores individuales que componen el vector (por ejemplo, las frecuencias de palabras en un vector de conteo, o los valores de características numéricas).\n",
    "\n",
    "**No, los elementos individuales de los vectores no tienen que estar normalizados previamente.**\n",
    "\n",
    "La normalización en el contexto de la similitud del coseno se refiere a la **normalización de la magnitud del vector completo**, no a la escala de sus componentes individuales. La fórmula de la similitud del coseno ya se encarga de este aspecto:\n",
    "\n",
    "$$\\text{similitud}(A, B) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|}$$\n",
    "\n",
    "Aquí, $\\|A\\|$ y $\\|B\\|$ son las **normas euclidianas (magnitudes) de los vectores completos**. Al dividir por estas magnitudes, el cálculo inherentemente \"normaliza\" los vectores para que su longitud no influya en la similitud. Esto significa que si tienes un vector $A = [1, 2, 3]$ y un vector $B = [2, 4, 6]$, sus elementos no están normalizados individualmente en el sentido de que no suman 1 ni están en un rango específico como [0, 1]. Sin embargo, la similitud del coseno entre ellos será 1, porque apuntan en la misma dirección, a pesar de que $B$ es el doble de \"grande\" que $A$.\n",
    "\n",
    "### Diferencia con otras normalizaciones\n",
    "\n",
    "Es importante no confundir esta propiedad con otras formas de normalización que se usan en preprocesamiento de datos, como:\n",
    "\n",
    "* **Normalización Min-Max:** Escala los valores a un rango específico (ej. [0, 1]).\n",
    "* **Estandarización (Z-score):** Transforma los datos para que tengan una media de 0 y una desviación estándar de 1.\n",
    "* **Normalización L1:** Escala el vector para que la suma de sus valores absolutos sea 1.\n",
    "\n",
    "Estas otras normalizaciones se aplican a los elementos de los vectores por diferentes razones (por ejemplo, para que un algoritmo de aprendizaje automático no se vea sesgado por la escala de ciertas características), pero **no son un requisito previo para que la similitud del coseno funcione correctamente**. La similitud del coseno está diseñada para funcionar directamente con los valores originales de los elementos, ya que su propia fórmula maneja la \"normalización de dirección\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6900e121",
   "metadata": {},
   "source": [
    "# ¿Puedo darle más peso a algunos elementos de los vectores que a otros en la similitud del coseno?\n",
    "\n",
    "---\n",
    "\n",
    "**Sí, puedes darle más peso a ciertos elementos de los vectores que a otros** al calcular la similitud del coseno. Esto se logra mediante una técnica llamada **ponderación** antes de aplicar la fórmula de la similitud del coseno.\n",
    "\n",
    "La similitud del coseno estándar trata a todos los elementos del vector por igual. Sin embargo, en muchas aplicaciones (especialmente en procesamiento de lenguaje natural o sistemas de recomendación), algunos atributos o términos pueden ser más importantes que otros para determinar la similitud.\n",
    "\n",
    "### ¿Cómo se aplica la ponderación?\n",
    "\n",
    "La ponderación se aplica multiplicando los elementos del vector por un factor de peso. Hay varias formas de hacerlo:\n",
    "\n",
    "1.  **Ponderación Directa de Elementos:**\n",
    "    Puedes multiplicar cada elemento del vector por un peso específico que refleje su importancia. Por ejemplo, si tienes vectores de características donde la característica `X` es más importante que la característica `Y`, simplemente multiplicas el valor de `X` por un peso mayor.\n",
    "\n",
    "    * **Ejemplo en PNL (TF-IDF):** Un método muy común es usar la ponderación TF-IDF (Term Frequency-Inverse Document Frequency). Aquí, los elementos del vector (que representan palabras) se ponderan no solo por su frecuencia en el documento (`TF`) sino también por su rareza o importancia en todo el corpus de documentos (`IDF`). Las palabras que aparecen en muchos documentos (y son menos distintivas, como \"el\", \"la\") obtienen un peso bajo de IDF, mientras que las palabras raras y distintivas obtienen un peso alto.\n",
    "\n",
    "2.  **Ponderación mediante una Matriz Diagonal:**\n",
    "    Si estás trabajando con transformaciones matriciales, podrías pensar en una matriz diagonal donde los elementos de la diagonal son los pesos. Al multiplicar tus vectores por esta matriz, estarías aplicando la ponderación. Sin embargo, para la mayoría de los casos prácticos, la ponderación directa de elementos es más sencilla.\n",
    "\n",
    "### Ejemplo conceptual con ponderación:\n",
    "\n",
    "Imagina que tienes dos documentos representados por vectores de conteo de palabras, y sabes que la palabra \"algoritmo\" es mucho más relevante que la palabra \"y\" para determinar la similitud temática.\n",
    "\n",
    "* **Vector Documento 1 (original):** `[frec_algoritmo, frec_y, frec_dato]`\n",
    "* **Vector Documento 2 (original):** `[frec_algoritmo, frec_y, frec_dato]`\n",
    "\n",
    "Si decides que \"algoritmo\" debe tener el doble de peso, aplicarías una ponderación así:\n",
    "\n",
    "* **Vector Documento 1 (ponderado):** `[frec_algoritmo * 2, frec_y * 1, frec_dato * 1]`\n",
    "* **Vector Documento 2 (ponderado):** `[frec_algoritmo * 2, frec_y * 1, frec_dato * 1]`\n",
    "\n",
    "Luego, calculas la similitud del coseno sobre estos **vectores ponderados**.\n",
    "\n",
    "### Consideraciones\n",
    "\n",
    "* **Justificación de los Pesos:** La clave es tener una buena razón o un método fundamentado (como TF-IDF) para determinar qué pesos asignar a cada elemento. Los pesos suelen derivarse del dominio del problema, análisis de datos o algoritmos de aprendizaje.\n",
    "* **Impacto en la Interpretación:** Al ponderar, el concepto de \"similitud direccional\" sigue siendo el mismo, pero ahora esa dirección está influenciada más fuertemente por los elementos a los que se les dio mayor peso.\n",
    "\n",
    "En conclusión, la ponderación es una técnica poderosa para refinar el cálculo de la similitud del coseno, permitiendo que la importancia relativa de las características influya en el resultado final."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930f4e8d",
   "metadata": {},
   "source": [
    "# Ejemplo en Python de ponderación de elementos en la similitud del coseno\n",
    "\n",
    "---\n",
    "\n",
    "Aquí te muestro cómo puedes aplicar ponderación a los elementos de tus vectores antes de calcular la similitud del coseno. Usaremos un ejemplo donde ciertos \"términos\" o \"características\" son más importantes que otros.\n",
    "\n",
    "Imagina que estás analizando la similitud entre dos reseñas de películas, y sabes que las palabras relacionadas con la \"trama\" son más importantes que las relacionadas con el \"casting\" o la \"banda sonora\" para determinar si las reseñas son similares en contenido.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c238a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Reseña 1 (original): [2 5 1 3 1]\n",
      "Vector Reseña 2 (original): [1 4 2 2 0]\n",
      "\n",
      "--- Similitud SIN PONDERACIÓN: 0.9487 ---\n",
      "\n",
      "Pesos aplicados: [1.  2.5 1.8 0.5 0.7]\n",
      "\n",
      "Vector Reseña 1 (ponderado): [ 2.  12.5  1.8  1.5  0.7]\n",
      "Vector Reseña 2 (ponderado): [ 1.  10.   3.6  1.   0. ]\n",
      "\n",
      "--- Similitud CON PONDERACIÓN: 0.9764 ---\n",
      "\n",
      "Análisis:\n",
      "- Sin ponderación, la similitud fue: 0.9487\n",
      "- Con ponderación, la similitud fue: 0.9764\n",
      "  La similitud con ponderación es mayor, lo que indica que las características importantes (trama, personajes) hicieron que las reseñas se percibieran como más similares.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Definimos dos vectores de ejemplo (representan reseñas de películas)\n",
    "# Supongamos que los elementos corresponden a:\n",
    "# [ 'acción', 'trama', 'personajes', 'casting', 'música' ]\n",
    "vector_reseña_1 = np.array([2, 5, 1, 3, 1])\n",
    "vector_reseña_2 = np.array([1, 4, 2, 2, 0])\n",
    "\n",
    "print(f\"Vector Reseña 1 (original): {vector_reseña_1}\")\n",
    "print(f\"Vector Reseña 2 (original): {vector_reseña_2}\")\n",
    "\n",
    "# --- Calculamos la similitud del coseno SIN ponderación ---\n",
    "def calcular_similitud_coseno(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "\n",
    "    if norm_vec1 == 0 or norm_vec2 == 0:\n",
    "        return 0.0\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "similitud_sin_ponderacion = calcular_similitud_coseno(vector_reseña_1, vector_reseña_2)\n",
    "print(f\"\\n--- Similitud SIN PONDERACIÓN: {similitud_sin_ponderacion:.4f} ---\\n\")\n",
    "\n",
    "# --- Aplicamos PONDERACIÓN a los elementos ---\n",
    "# Queremos darle más peso a 'trama' (índice 1) y 'personajes' (índice 2)\n",
    "# y menos peso a 'casting' (índice 3) y 'música' (índice 4).\n",
    "# Los pesos deben ser arrays numpy para la multiplicación elemento a elemento.\n",
    "# [ 'acción', 'trama', 'personajes', 'casting', 'música' ]\n",
    "pesos = np.array([1.0, 2.5, 1.8, 0.5, 0.7]) # Ajusta estos pesos según tu necesidad\n",
    "\n",
    "print(f\"Pesos aplicados: {pesos}\")\n",
    "\n",
    "# Aplicar los pesos a cada vector multiplicando elemento a elemento\n",
    "vector_reseña_1_ponderado = vector_reseña_1 * pesos\n",
    "vector_reseña_2_ponderado = vector_reseña_2 * pesos\n",
    "\n",
    "print(f\"\\nVector Reseña 1 (ponderado): {vector_reseña_1_ponderado}\")\n",
    "print(f\"Vector Reseña 2 (ponderado): {vector_reseña_2_ponderado}\")\n",
    "\n",
    "# --- Calculamos la similitud del coseno CON ponderación ---\n",
    "similitud_con_ponderacion = calcular_similitud_coseno(vector_reseña_1_ponderado, vector_reseña_2_ponderado)\n",
    "print(f\"\\n--- Similitud CON PONDERACIÓN: {similitud_con_ponderacion:.4f} ---\\n\")\n",
    "\n",
    "# --- Análisis del resultado ---\n",
    "print(\"Análisis:\")\n",
    "print(f\"- Sin ponderación, la similitud fue: {similitud_sin_ponderacion:.4f}\")\n",
    "print(f\"- Con ponderación, la similitud fue: {similitud_con_ponderacion:.4f}\")\n",
    "if similitud_con_ponderacion > similitud_sin_ponderacion:\n",
    "    print(\"  La similitud con ponderación es mayor, lo que indica que las características importantes (trama, personajes) hicieron que las reseñas se percibieran como más similares.\")\n",
    "elif similitud_con_ponderacion < similitud_sin_ponderacion:\n",
    "    print(\"  La similitud con ponderación es menor, lo que indica que las características importantes hicieron que las reseñas se percibieran como menos similares.\")\n",
    "else:\n",
    "    print(\"  La ponderación no alteró significativamente la similitud en este caso específico.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
